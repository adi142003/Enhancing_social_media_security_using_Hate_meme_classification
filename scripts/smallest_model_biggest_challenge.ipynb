{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, ViTModel, RobertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file='rationale.csv', csv_file2='train.csv', transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data2 = pd.read_csv(csv_file2)\n",
    "        self.data = self.data[:50]\n",
    "        self.data2 = self.data2[:50]\n",
    "        self.transform = transform\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data['image_path'][idx]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        ocr = self.data2['ocr'][idx]\n",
    "        ocr_tokens = self.tokenizer(ocr, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        \n",
    "        llm_rationale = self.data['rationale'][idx]\n",
    "        rationale_tokens = self.tokenizer(llm_rationale, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        \n",
    "        return image, ocr_tokens, rationale_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, text_hidden_size=768, image_hidden_size=768, fusion_output_size=512):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.text_encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        \n",
    "        self.fusion = nn.Linear(text_hidden_size + image_hidden_size, fusion_output_size)\n",
    "        self.rationale_generator = nn.Linear(fusion_output_size, text_hidden_size)\n",
    "        \n",
    "    def forward(self, text_tokens, image):\n",
    "        text_features = self.text_encoder(**text_tokens).last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "        image_features = self.image_encoder(image).last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "        \n",
    "        fused_features = torch.cat((text_features, image_features), dim=1)\n",
    "        fused_features = self.fusion(fused_features)\n",
    "        rationale_features = self.rationale_generator(fused_features)\n",
    "        \n",
    "        return rationale_features\n",
    "\n",
    "def knowledge_distillation_loss(student_features, teacher_features, temperature=1.0):\n",
    "    return nn.MSELoss()(student_features, teacher_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knowledge_distillation_loss(student_logits, teacher_logits, temperature=1.0):\n",
    "#     return nn.KLDivLoss()(F.log_softmax(student_logits / temperature, dim=1),\n",
    "#                           F.softmax(teacher_logits / temperature, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('rationale.csv', 'train.csv', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()  # For simplicity, using MSE loss for rationale regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images, ocr_tokens, llm_rationale_tokens = batch\n",
    "        images = images.to(device)\n",
    "        ocr_tokens = {k: v.squeeze(1).to(device) for k, v in ocr_tokens.items()}\n",
    "        llm_rationale_tokens = {k: v.squeeze(1).to(device) for k, v in llm_rationale_tokens.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        model_rationale_features = model(ocr_tokens, images)\n",
    "        llm_rationale_features = model.text_encoder(**llm_rationale_tokens).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Calculate losses\n",
    "        kd_loss = knowledge_distillation_loss(model_rationale_features, llm_rationale_features)\n",
    "        \n",
    "        total_loss = kd_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_temp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Rationale:\n",
      " taken\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_rationale(model, image_path, ocr_text, device, tokenizer):\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # OCR text preprocessing\n",
    "    ocr_tokens = tokenizer(ocr_text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    ocr_tokens = {k: v.to(device) for k, v in ocr_tokens.items()}\n",
    "\n",
    "    # Generate rationale\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        rationale_features = model(ocr_tokens, image)\n",
    "    \n",
    "    # Convert features to text (this step depends on your model's output)\n",
    "    # Assuming the model's output is logits for each token in the vocabulary\n",
    "    predicted_token_ids = torch.argmax(rationale_features, dim=-1)\n",
    "    generated_rationale=\"\"\n",
    "    for i in predicted_token_ids:\n",
    "        generated_rationale += tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_rationale\n",
    "\n",
    "# Load your trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModel().to(device)\n",
    "model.load_state_dict(torch.load('model_temp.pth'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "df = pd.read_csv('val.csv',delimiter='\\t')\n",
    "# Example usage\n",
    "image_path = './valImages/2.jpg'\n",
    "ocr_text = df['ocr'][2]\n",
    "\n",
    "generated_rationale = generate_rationale(model, image_path, ocr_text, device, tokenizer)\n",
    "print(\"Generated Rationale:\")\n",
    "print(generated_rationale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Load validation data\n",
    "# dataset = CustomDataset('val_rationale.csv', 'val.csv', transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Tokenizer for decoding\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# # Evaluate the model on the validation set\n",
    "# with torch.no_grad():\n",
    "#     for batch in dataloader:\n",
    "#         images, ocr_tokens, llm_rationale_tokens = batch\n",
    "#         images = images.to(device)\n",
    "#         ocr_tokens = {k: v.squeeze(1).to(device) for k, v in ocr_tokens.items()}\n",
    "#         llm_rationale_tokens = {k: v.squeeze(1).to(device) for k, v in llm_rationale_tokens.items()}\n",
    "        \n",
    "#         # Forward pass\n",
    "#         model_rationale_features = model(ocr_tokens, images)\n",
    "        \n",
    "#         # If model_rationale_features are logits, convert to token IDs\n",
    "#         model_rationale_token_ids = torch.argmax(model_rationale_features, dim=-1)\n",
    "        \n",
    "#         # Decode token IDs to text\n",
    "#         decoded_rationale = tokenizer.decode(model_rationale_token_ids.squeeze().cpu().numpy(), skip_special_tokens=False)\n",
    "        \n",
    "#         # Print the decoded rationale text\n",
    "#         print(decoded_rationale)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
