{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, ViTFeatureExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, ViTModel, T5ForConditionalGeneration\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, csv_file, csv_file2, image_dir, tokenizer, feature_extractor, max_text_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with image paths and LLaVA rationales.\n",
    "            csv_file2 (str): Path to the CSV file with OCR text.\n",
    "            image_dir (str): Directory with all the meme images.\n",
    "            tokenizer (transformers tokenizer): Tokenizer for OCR text.\n",
    "            feature_extractor (transformers feature extractor): Feature extractor for images.\n",
    "            max_text_length (int): Maximum length for text tokens.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)  # CSV with image paths and rationales\n",
    "        self.data2 = pd.read_csv(csv_file2)  # CSV with OCR text\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_text_length = max_text_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract relevant data using `idx`\n",
    "        image_path = os.path.join(self.image_dir, str(self.data.iloc[idx]['image_path']))  # Get image path for idx\n",
    "        ocr_text = str(self.data2.iloc[idx]['ocr'])  # Get OCR text for idx\n",
    "        target_explanation = str(self.data.iloc[idx]['rationale'])  # Get LLaVA rationale for idx\n",
    "\n",
    "        # Process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.feature_extractor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "\n",
    "        # Process text (OCR text)\n",
    "        text_encoding = self.tokenizer(\n",
    "            ocr_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Process target explanation (rationale)\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_explanation,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'ocr_text_input_ids': text_encoding['input_ids'].squeeze(),\n",
    "            'ocr_text_attention_mask': text_encoding['attention_mask'].squeeze(),\n",
    "            'target_ids': target_encoding['input_ids'].squeeze(),\n",
    "            'target_attention_mask': target_encoding['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "# dataset = MemeDataset('path_to_csv.csv', 'path_to_csv2.csv', 'path_to_images/', tokenizer, feature_extractor)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalExplanationModel(nn.Module):\n",
    "    def __init__(self, text_model_name='roberta-base', vision_model_name='google/vit-base-patch16-224', t5_model_name='t5-small'):\n",
    "        super(MultimodalExplanationModel, self).__init__()\n",
    "        \n",
    "        # Text and Vision encoders\n",
    "        self.text_encoder = RobertaModel.from_pretrained(text_model_name)\n",
    "        self.vision_encoder = ViTModel.from_pretrained(vision_model_name)\n",
    "        \n",
    "        # Decoder for generating explanations\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "        # Linear layer to combine the two modalities\n",
    "        self.fc = nn.Linear(self.text_encoder.config.hidden_size + self.vision_encoder.config.hidden_size, self.decoder.config.d_model)\n",
    "\n",
    "    def forward(self, ocr_text_input_ids, ocr_text_attention_mask, image, target_ids=None, target_attention_mask=None):\n",
    "    # Textual features\n",
    "        text_outputs = self.text_encoder(input_ids=ocr_text_input_ids, attention_mask=ocr_text_attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use the <s> token (CLS token) for classification\n",
    "        \n",
    "        # Visual features\n",
    "        vision_outputs = self.vision_encoder(pixel_values=image)\n",
    "        vision_features = vision_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token for classification\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat((text_features, vision_features), dim=1)\n",
    "        combined_features = self.fc(combined_features)\n",
    "        \n",
    "        # Repeat the combined features to create a pseudo-sequence\n",
    "        repeated_features = combined_features.unsqueeze(1).repeat(1, target_ids.size(1), 1)  # Repeat along the sequence length\n",
    "        \n",
    "        # Generate explanation\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            encoder_outputs=(repeated_features,),  # Pass the repeated features as the encoder output\n",
    "            labels=target_ids\n",
    "        )\n",
    "\n",
    "        return decoder_outputs.loss, decoder_outputs.logits\n",
    "\n",
    "    \n",
    "    # def forward(self, ocr_text_input_ids, ocr_text_attention_mask, image, target_ids=None, target_attention_mask=None):\n",
    "    #     # Textual features\n",
    "    #     text_outputs = self.text_encoder(input_ids=ocr_text_input_ids, attention_mask=ocr_text_attention_mask)\n",
    "    #     text_features = text_outputs.last_hidden_state[:, 0, :]  # Use the <s> token (CLS token) for classification\n",
    "        \n",
    "    #     # Visual features\n",
    "    #     vision_outputs = self.vision_encoder(pixel_values=image)\n",
    "    #     vision_features = vision_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token for classification\n",
    "        \n",
    "    #     # Combine features\n",
    "    #     combined_features = torch.cat((text_features, vision_features), dim=1)\n",
    "    #     combined_features = self.fc(combined_features)\n",
    "        \n",
    "    #     # Generate explanation\n",
    "    #     decoder_outputs = self.decoder(\n",
    "    #         input_ids=target_ids,\n",
    "    #         attention_mask=target_attention_mask,\n",
    "    #         encoder_outputs=(combined_features.unsqueeze(1),),\n",
    "    #         labels=target_ids\n",
    "    #     )\n",
    "\n",
    "    #     return decoder_outputs.loss, decoder_outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\aimlc\\videos\\text-sql\\.conda\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\aimlc\\Videos\\text-sql\\.conda\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Ensure the tokenizer is correctly set up\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = MultimodalExplanationModel()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Data\n",
    "dataset = MemeDataset('./DATA/scripts/rationale.csv','./DATA/scripts/train.csv', './DATA', tokenizer, feature_extractor)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in tqdm(dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Get data\n",
    "#         print(\n",
    "#         ocr_text_input_ids = batch['ocr_text_input_ids'],\n",
    "#         ocr_text_attention_mask = batch['ocr_text_attention_mask'],\n",
    "#         image = batch['image'],\n",
    "#         target_ids = batch['target_ids'],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample rationale**\n",
    "Generated Explanation: Target Group or Person: The meme targets a specific individual, Narendra Modi, who is a political figure. It references his actions and the implication that he is not in a position of influence. Content Evaluation: The text is potentially offensive due to its reference to a political figure's actions and the use of a political figure to satirize the situation. Context and Implications: The context is political commentary on the political climate, which may be seen as a political commentary on the political climate. Overall Assessment: The meme uses humor to comment on political issues and political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [1:03:47<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 3.57098759160723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [1:06:38<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 2.8444256003243584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [1:02:35<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 2.705521892820086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:42<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 2.6137118225097655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:47<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 2.5509440326690673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:44<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 2.497531508854457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:38<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 2.4435541902269637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:40<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 2.389111866269793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [50:03<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 2.3292890570504325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [49:41<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 2.26340590436118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Set the number of epochs\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get data\n",
    "        ocr_text_input_ids = batch['ocr_text_input_ids']\n",
    "        ocr_text_attention_mask = batch['ocr_text_attention_mask']\n",
    "        image = batch['image']\n",
    "        target_ids = batch['target_ids']\n",
    "        target_attention_mask = batch['target_attention_mask']\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, logits = model(\n",
    "            ocr_text_input_ids=ocr_text_input_ids,\n",
    "            ocr_text_attention_mask=ocr_text_attention_mask,\n",
    "            image=image,\n",
    "            target_ids=target_ids,\n",
    "            target_attention_mask=target_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{10} - Loss: {epoch_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        ocr_text_input_ids = batch['ocr_text_input_ids']\n",
    "        ocr_text_attention_mask = batch['ocr_text_attention_mask']\n",
    "        image = batch['image']\n",
    "\n",
    "        # Get text and vision features\n",
    "        text_features = model.text_encoder(ocr_text_input_ids, attention_mask=ocr_text_attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_features = model.vision_encoder(pixel_values=image).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Concatenate and pass through the linear layer\n",
    "        combined_features = model.fc(torch.cat((text_features, vision_features), dim=1))\n",
    "        \n",
    "        # Repeat features to simulate sequence and wrap in BaseModelOutput\n",
    "        repeated_features = combined_features.unsqueeze(1).repeat(1, 1, 1)\n",
    "        encoder_outputs = BaseModelOutput(last_hidden_state=repeated_features)\n",
    "\n",
    "        # Generate explanation using T5 generate method\n",
    "        generated_ids = model.decoder.generate(\n",
    "            input_ids=None, \n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=150\n",
    "        )\n",
    "\n",
    "        generated_explanation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(\"Generated Explanation:\", generated_explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'multimodal_explanation_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('multimodal_explanation_model.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
