{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, ToPILImage\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"./DATA/trainImages\"\n",
    "val_images = \"./DATA/valImages\"\n",
    "test_images = \"./DATA/testImages\"\n",
    "\n",
    "train_roi_jsonl = \"./DATA/data.jsonl\"\n",
    "val_roi_jsonl = \"./DATA/val.jsonl\"\n",
    "test_roi_jsonl = \"./DATA/test.jsonl\"\n",
    "\n",
    "\n",
    "train_csv = \"./DATA/train.csv\"\n",
    "val_csv = \"./DATA/val.csv\"\n",
    "test_csv = \"./DATA/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import json\n",
    "\n",
    "model = YOLO('yolov8x.pt')  \n",
    "\n",
    "data = []\n",
    "images_path = './DATA/testImages'\n",
    "\n",
    "ls = os.listdir(images_path)\n",
    "ls.sort(key=lambda x: int(x.split(\".\")[0]))\n",
    "for i, image_path in enumerate(ls):\n",
    "    # Read the image\n",
    "    image = cv2.imread(os.path.join(images_path, image_path))\n",
    "    results = model(image)\n",
    "    res = []\n",
    "    # Extract bounding boxes, labels, and confidence scores\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Extract bounding box coordinates\n",
    "        scores = result.boxes.conf.cpu().numpy()  # Extract confidence scores\n",
    "        labels = result.boxes.cls.cpu().numpy()  # Extract class labels\n",
    "        # Draw bounding boxes on the image\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name = model.names[int(label)]\n",
    "            # cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label_text = f'{class_name} {score:.2f}'\n",
    "            # cv2.putText(image, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            res.append({\n",
    "                \"label\": label_text,\n",
    "                \"class_name\": class_name,\n",
    "                \"box\": [[x1, y1], [x2, y2]]\n",
    "            })\n",
    "    data.append({\n",
    "        \"id\": image_path.split(\"/\")[-1],\n",
    "        \"image\": image_path,\n",
    "        \"boxes\": res\n",
    "    })\n",
    "    \n",
    "with open(\"test.jsonl\", 'w') as f:\n",
    "    for obj in data:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda/envs/CAPSTONE/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "clip_model = torch.jit.load(\"./models/clip_model.pt\").cuda().eval()\n",
    "input_resolution = clip_model.input_resolution.item()\n",
    "context_length = clip_model.context_length.item()\n",
    "vocab_size = clip_model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "preprocess = Compose([\n",
    "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(input_resolution),\n",
    "    ToTensor()\n",
    "    ])\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base').cuda().eval()\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Preprocess image for CLIP\n",
    "def process_image_clip(in_img):\n",
    "    image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
    "    image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])\n",
    "    \n",
    "    image = preprocess(Image.open(in_img).convert(\"RGB\"))\n",
    "    \n",
    "    image_input = torch.tensor(np.stack(image))\n",
    "    image_input -= image_mean[:, None, None]\n",
    "    image_input /= image_std[:, None, None]\n",
    "    return image_input\n",
    "\n",
    "\n",
    "# Preprocess text using RoBERTa\n",
    "def process_text_roberta(in_text, max_length=77):\n",
    "    # tokens = roberta_tokenizer(in_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=75)\n",
    "    # tokens = {k: v.to(\"cuda\") for k, v in tokens.items()}\n",
    "    # text_embedding = roberta_model(**tokens).last_hidden_state\n",
    "    # return text_embedding\n",
    "    return roberta_tokenizer(\n",
    "                    text=in_text, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding='max_length', \n",
    "                    max_length=max_length, \n",
    "                    truncation=True\n",
    "                ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Extract VGG-16 Feature Layers\n",
    "        self.features = list(model.features)\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        # Extract VGG-16 Average Pooling Layer\n",
    "        self.pooling = model.avgpool\n",
    "        # Convert the image into one-dimensional vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Extract the first part of fully-connected layer from VGG16\n",
    "        self.fc = model.classifier[0]\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # It will take the input 'x' until it returns the feature vector called 'out'\n",
    "        out = self.features(x)\n",
    "        out = self.pooling(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out) \n",
    "        return out \n",
    "\n",
    "# Initialize the model\n",
    "model_vgg_pretrained = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "model_vgg = FeatureExtractor(model_vgg_pretrained)\n",
    "\n",
    "# Change the device to GPU\n",
    "model_vgg = model_vgg.to(device)\n",
    "\n",
    "# Transform the image, so it becomes readable with the model\n",
    "transform_vgg_BB = Compose([\n",
    "  ToPILImage(),\n",
    "  CenterCrop(512),\n",
    "  Resize((448,448)),\n",
    "  ToTensor()                              \n",
    "])\n",
    "\n",
    "\n",
    "# Iterate each image\n",
    "def get_image_vgg_BB(l, t, r, b, in_im): \n",
    "#     left, top, right, bottom and input image\n",
    "    img = cv2.imread(in_im)\n",
    "    h, w, _ = img.shape\n",
    "    # crop\n",
    "    x1 = int(np.floor(l*w))\n",
    "    x2 = int(np.floor(r*w))\n",
    "    y1 = int(np.floor(b*h))\n",
    "    y2 = int(np.floor(t*h))\n",
    "    crop_img = img[y1:y2, x1:x2]    \n",
    "    \n",
    "    # Transform the cropped image\n",
    "    img = transform_vgg_BB(crop_img)\n",
    "    # Reshape the image. PyTorch model reads 4-dimensional tensor\n",
    "    # [batch_size, channels, width, height]\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    img = img.to(device)\n",
    "    # We only extract features, so we don't need gradient\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = model_vgg(img).squeeze()\n",
    "    # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "    return feature\n",
    "\n",
    "# Transform the image, so it becomes readable with the model\n",
    "transform_vgg_center = Compose([\n",
    "  ToPILImage(),\n",
    "  CenterCrop(512),\n",
    "  Resize(448),\n",
    "  ToTensor()                              \n",
    "])\n",
    "\n",
    "# Iterate each image\n",
    "def get_image_vgg_center(in_im):\n",
    "    # Set the image path\n",
    "    path = in_im\n",
    "    # Read the file\n",
    "    img = cv2.imread(path)\n",
    "    # Transform the image\n",
    "    img = transform_vgg_center(img)\n",
    "    # Reshape the image. PyTorch model reads 4-dimensional tensor\n",
    "    # [batch_size, channels, width, height]\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    img = img.to(device)\n",
    "    # We only extract features, so we don't need gradient\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = model_vgg(img).squeeze()\n",
    "    \n",
    "    return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/CAPSTONE/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_sent_trans = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').cuda().eval()\n",
    "\n",
    "class DatasetCreater(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class to preprocess and serve multimodal tensors for model input.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        data_path_roi=None,\n",
    "        split_flag=None,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "        self.split_flag = split_flag\n",
    "        \n",
    "        delimeter = '\\t' if split_flag == \"val\" else \",\"\n",
    "        \n",
    "        self.samples_frame = pd.read_csv(data_path, delimiter=delimeter)\n",
    "        \n",
    "        if data_path_roi is not None:\n",
    "            self.roi_frame = pd.read_json(data_path_roi, lines=True)\n",
    "            self.roi_frame.image = self.roi_frame.apply(lambda row: (img_dir + '/' + row[\"image\"]), axis=1)\n",
    "            \n",
    "        self.samples_frame = self.samples_frame.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_id = self.roi_frame.loc[idx, \"id\"]\n",
    "        img_file_name = self.roi_frame.loc[idx, \"image\"]\n",
    "\n",
    "        # Process image using CLIP's image processor\n",
    "        image_clip_input = process_image_clip(self.roi_frame.loc[idx, \"image\"])\n",
    "\n",
    "        # On-demand VGG feature extraction or pre-extracted\n",
    "        BB_info = self.roi_frame.loc[idx, \"boxes\"]\n",
    "        roi_vgg_feat_list = []\n",
    "        if len(BB_info):\n",
    "            total_BB = len(BB_info)\n",
    "            BB_info_final = BB_info[:4] if total_BB > 4 else BB_info\n",
    "            for item in BB_info_final:\n",
    "                left = item['box'][0][0]\n",
    "                top = item['box'][0][1]\n",
    "                right = item['box'][1][0]\n",
    "                bottom = item['box'][1][1]\n",
    "                roi_vgg_feat = get_image_vgg_BB(left, top, right, bottom, img_file_name)\n",
    "                roi_vgg_feat_list.append(roi_vgg_feat)\n",
    "            image_vgg_feature = torch.mean(torch.vstack(roi_vgg_feat_list), axis=0).to('cuda')\n",
    "        else:\n",
    "            image_vgg_feature = get_image_vgg_center(img_file_name).clone().detach().to('cuda')\n",
    "\n",
    "        # Process text using RoBERTa\n",
    "        text_roberta_input = process_text_roberta(self.samples_frame.loc[idx, \"ocr\"])\n",
    "\n",
    "        # Process entities (entity embeddings with sentence-transformers)\n",
    "        cur_ent_list = [item[\"class_name\"] for item in self.roi_frame.loc[idx, \"boxes\"]]\n",
    "        cur_ent_rep_list = [torch.tensor(model_sent_trans.encode(item)).to(device) for item in cur_ent_list]\n",
    "        text_drob_feature = torch.mean(torch.vstack(cur_ent_rep_list), axis=0).to(device) if len(cur_ent_list) else torch.tensor(model_sent_trans.encode(self.samples_frame.loc[idx, \"ocr\"])).to(device)\n",
    "\n",
    "        if self.split_flag == 'test':\n",
    "            sample = {\n",
    "                \"id\": img_id,\n",
    "                \"image_clip_input\": image_clip_input.to(\"cuda\"),\n",
    "                \"image_vgg_feature\": image_vgg_feature,\n",
    "                \"text_roberta_input\": text_roberta_input.to(\"cuda\"),\n",
    "                \"text_drob_embedding\": text_drob_feature,\n",
    "            }\n",
    "        else:    \n",
    "            lab = 0 if self.samples_frame.loc[idx, \"offensive\"] == \"not_offensive\" else 1\n",
    "            label = torch.tensor(lab).to('cuda')\n",
    "            sample = {\n",
    "                \"id\": img_id,\n",
    "                \"image_clip_input\": image_clip_input.to(\"cuda\"),\n",
    "                \"image_vgg_feature\": image_vgg_feature,\n",
    "                \"text_roberta_input\": text_roberta_input.to(\"cuda\"),\n",
    "                \"text_drob_embedding\": text_drob_feature,\n",
    "                \"label\": label\n",
    "            }\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset_train = DatasetCreater(train_csv, train_images, data_path_roi=train_roi_jsonl, split_flag='train')\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "dataset_val = DatasetCreater(val_csv, val_images, data_path_roi=val_roi_jsonl ,split_flag='val')\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "dataset_test = DatasetCreater(test_csv, test_images, data_path_roi=test_roi_jsonl, split_flag='test')\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32,\n",
    "                        shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Momenta(nn.Module):\n",
    "    def __init__(self, n_out):\n",
    "        super(Momenta, self).__init__()  \n",
    "        \n",
    "        # Dense layers for VGG and RoBERTa features\n",
    "        self.dense_vgg_1024 = nn.Linear(4096, 1024)\n",
    "        self.dense_vgg_512 = nn.Linear(1024, 512)\n",
    "        self.drop20 = nn.Dropout(p=0.2)\n",
    "        self.drop5 = nn.Dropout(p=0.05) \n",
    "        \n",
    "        self.dense_drob_512 = nn.Linear(768, 512)\n",
    "        \n",
    "        # Self-attention layers\n",
    "        self.gen_key_L1 = nn.Linear(512, 256) \n",
    "        self.gen_query_L1 = nn.Linear(512, 256)\n",
    "        self.gen_middle_L2 = nn.Linear(768, 512)\n",
    "        self.gen_key_L2 = nn.Linear(512, 256)\n",
    "        self.gen_query_L2 = nn.Linear(512, 256) \n",
    "        self.gen_key_L3 = nn.Linear(512, 256)\n",
    "        self.gen_query_L3 = nn.Linear(512, 256)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.project_dense_512a = nn.Linear(1024, 512)\n",
    "        self.project_dense_512b = nn.Linear(1024, 512)\n",
    "        self.project_dense_512c = nn.Linear(1024, 512)\n",
    "        \n",
    "        # Final layers\n",
    "        self.fc_out = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_out)\n",
    "        \n",
    "\n",
    "    def selfattNFuse_L1a(self, vec1, vec2): \n",
    "        q1 = F.relu(self.gen_query_L1(vec1))\n",
    "        k1 = F.relu(self.gen_key_L1(vec1))\n",
    "        q2 = F.relu(self.gen_query_L1(vec2))\n",
    "        k2 = F.relu(self.gen_key_L1(vec2))\n",
    "        score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "        score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "        wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "        wt_i1_i2 = self.soft(wt_score1_score2_mat.float())\n",
    "        prob_1 = wt_i1_i2[:, 0]\n",
    "        prob_2 = wt_i1_i2[:, 1]\n",
    "        wtd_i1 = vec1 * prob_1[:, None]\n",
    "        wtd_i2 = vec2 * prob_2[:, None]\n",
    "        out_rep = F.relu(self.project_dense_512a(torch.cat((wtd_i1, wtd_i2), 1)))\n",
    "        return out_rep\n",
    "\n",
    "    def selfattNFuse_L1b(self, vec1, vec2):\n",
    "        vec2_pool = torch.mean(vec2, dim=1).to(device)\n",
    "        q1 = F.relu(self.gen_query_L2(vec1))\n",
    "        k1 = F.relu(self.gen_key_L2(vec1))\n",
    "        q2a = self.gen_middle_L2(vec2_pool)\n",
    "        q2 = F.relu(self.gen_query_L2(q2a))\n",
    "        k2 = F.relu(self.gen_key_L2(q2a))\n",
    "        score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "        score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "        wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "        wt_i1_i2 = self.soft(wt_score1_score2_mat.float())\n",
    "        prob_1 = wt_i1_i2[:, 0]\n",
    "        prob_2 = wt_i1_i2[:, 1]\n",
    "        wtd_i1 = vec1 * prob_1[:, None]\n",
    "        wtd_i2 = q2a * prob_2[:, None]\n",
    "        out_rep = F.relu(self.project_dense_512b(torch.cat((wtd_i1, wtd_i2), 1)))\n",
    "        return out_rep\n",
    "    \n",
    "    def selfattNFuse_L2(self, vec1, vec2): \n",
    "        q1 = F.relu(self.gen_query_L3(vec1))\n",
    "        k1 = F.relu(self.gen_key_L3(vec1))\n",
    "        q2 = F.relu(self.gen_query_L3(vec2))\n",
    "        k2 = F.relu(self.gen_key_L3(vec2))\n",
    "        score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "        score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "        wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "        wt_i1_i2 = self.soft(wt_score1_score2_mat.float())\n",
    "        prob_1 = wt_i1_i2[:, 0]\n",
    "        prob_2 = wt_i1_i2[:, 1]\n",
    "        wtd_i1 = vec1 * prob_1[:, None]\n",
    "        wtd_i2 = vec2 * prob_2[:, None]\n",
    "        out_rep = F.relu(self.project_dense_512c(torch.cat((wtd_i1, wtd_i2), 1)))\n",
    "        return out_rep\n",
    "\n",
    "    def forward(self, in_CI, in_VGG, in_CT, in_Drob):        \n",
    "        VGG_feat = self.drop20(F.relu(self.dense_vgg_512(self.drop20(F.relu(self.dense_vgg_1024(in_VGG))))))\n",
    "        Drob_feat = self.drop5(F.relu(self.dense_drob_512(in_Drob)))\n",
    "        out_img = self.selfattNFuse_L1a(VGG_feat, in_CI)\n",
    "        out_txt = self.selfattNFuse_L1b(Drob_feat, in_CT)        \n",
    "        out_img_txt = self.selfattNFuse_L2(out_img, out_txt)\n",
    "        final_out = F.relu(self.fc_out(out_img_txt))\n",
    "        out = self.out(final_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from early_stopping_pytorch.pytorchtools import EarlyStopping\n",
    "\n",
    "def train_model(model, patience, n_epochs, dataloader_train, dataloader_val, clip_model, optimizer, criterion, exp_path, device):\n",
    "    epochs = n_epochs\n",
    "    train_acc_list, val_acc_list, train_loss_list, val_loss_list = [], [], [], []\n",
    "    \n",
    "    # Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
    "    chk_file = os.path.join(exp_path, 'checkpoint.pt')\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.cuda().train()\n",
    "        total_loss_train, total_train, correct_train = 0, 0, 0\n",
    "\n",
    "        for data in tqdm(dataloader_train):\n",
    "            img_inp_clip = data['image_clip_input'].to(device)\n",
    "            txt_inp_clip = data['text_roberta_input'].to(device)\n",
    "            \n",
    "            input_ids = txt_inp_clip['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = txt_inp_clip['attention_mask'].squeeze(1).to(device)\n",
    "            # img_feat_clip = clip_model(pixel_values=img_inp_clip).image_embeds\n",
    "            # txt_feat_clip = clip_model(input_ids=txt_inp_clip).text_embeds\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
    "                txt_feat_clip = roberta_model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask\n",
    "                ).last_hidden_state.to(device)\n",
    "\n",
    "                # img_feat_clip = clip_model(img_inp_clip).float().to(device)\n",
    "                # txt_feat_clip = clip_model(txt_inp_clip).float().to(device)\n",
    "            \n",
    "            \n",
    "            img_feat_vgg = data['image_vgg_feature'].to(device)\n",
    "            txt_feat_trans = data['text_drob_embedding'].to(device)\n",
    "            label_train = data['label'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            output = model(img_feat_clip, img_feat_vgg, txt_feat_clip, txt_feat_trans)\n",
    "            loss = criterion(output, label_train)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(\".\", end= ')\n",
    "            \n",
    "            _, predicted_train = torch.max(output, 1)\n",
    "            total_train += label_train.size(0)\n",
    "            correct_train += (predicted_train == label_train).sum().item()\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_loss = total_loss_train / total_train\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        model.cuda().eval()\n",
    "        total_loss_val, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(dataloader_val):\n",
    "                img_inp_clip = data['image_clip_input'].to(device)\n",
    "                txt_inp_clip = data['text_roberta_input'].to(device)\n",
    "                \n",
    "                input_ids = txt_inp_clip['input_ids'].squeeze(1).to(device)\n",
    "                attention_mask = txt_inp_clip['attention_mask'].squeeze(1).to(device)\n",
    "                # img_feat_clip = clip_model(pixel_values=img_inp_clip).image_embeds\n",
    "                # txt_feat_clip = clip_model(input_ids=txt_inp_clip).text_embeds\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
    "                    txt_feat_clip = roberta_model(\n",
    "                                input_ids=input_ids,\n",
    "                                attention_mask=attention_mask\n",
    "                    ).last_hidden_state.to(device)\n",
    "\n",
    "                # img_feat_clip = clip_model(img_inp_clip).float().to(device)\n",
    "                # txt_feat_clip = clip_model(txt_inp_clip).float().to(device)\n",
    "            \n",
    "                img_feat_vgg = data['image_vgg_feature'].to(device)\n",
    "                txt_feat_trans = data['text_drob_embedding'].to(device)\n",
    "                label_val = data['label'].to(device)\n",
    "\n",
    "                output = model(img_feat_clip, img_feat_vgg, txt_feat_clip, txt_feat_trans)\n",
    "                val_loss = criterion(output, label_val)\n",
    "\n",
    "                _, predicted_val = torch.max(output, 1)\n",
    "                total_val += label_val.size(0)\n",
    "                correct_val += (predicted_val == label_val).sum().item()\n",
    "                total_loss_val += val_loss.item()\n",
    "                \n",
    "        print(\"\\nSaving model...\") \n",
    "        torch.save(model.state_dict(), os.path.join(exp_path, \"momenta_1.pt\"))\n",
    "\n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        val_loss = total_loss_val / total_val\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
    "        \n",
    "        # model.train()\n",
    "\n",
    "    return model, train_acc_list, val_acc_list, train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 2\n",
    "patience = 10\n",
    "n_epochs = 25\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_exp_path = \"models/\"\n",
    "\n",
    "\n",
    "model = Momenta(output_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "model, train_acc_list, val_acc_list, train_loss_list, val_lost_lost = train_model(\n",
    "                                                                model, patience, n_epochs, \n",
    "                                                                dataloader_train, dataloader_val, \n",
    "                                                                clip_model, optimizer, criterion, model_exp_path, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "6\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./models/momenta_1.pt\"))\n",
    "\n",
    "\n",
    "data = iter(dataloader_test)\n",
    "data = next(data)\n",
    "# print(data)\n",
    "img_inp_clip = data['image_clip_input'].to(device)\n",
    "txt_inp_clip = data['text_roberta_input'].to(device)\n",
    "\n",
    "input_ids = txt_inp_clip['input_ids'].squeeze(1).to(device)\n",
    "attention_mask = txt_inp_clip['attention_mask'].squeeze(1).to(device)\n",
    "# img_feat_clip = clip_model(pixel_values=img_inp_clip).image_embeds\n",
    "# txt_feat_clip = clip_model(input_ids=txt_inp_clip).text_embeds\n",
    "\n",
    "model.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
    "    txt_feat_clip = roberta_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "    ).last_hidden_state.to(device)\n",
    "\n",
    "# img_feat_clip = clip_model(img_inp_clip).float().to(device)\n",
    "# txt_feat_clip = clip_model(txt_inp_clip).float().to(device)\n",
    "\n",
    "img_feat_vgg = data['image_vgg_feature'].to(device)\n",
    "txt_feat_trans = data['text_drob_embedding'].to(device)\n",
    "\n",
    "output = model(img_feat_clip, img_feat_vgg, txt_feat_clip, txt_feat_trans)\n",
    "label = torch.tensor(1).to('cuda')\n",
    "_, predicted_val = torch.max(output, 1)\n",
    "correct_val = (predicted_val == label).sum().item()\n",
    "print(predicted_val)\n",
    "print(correct_val)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.9377e-02,  6.8642e-01, -1.4575e+01],\n",
       "        [ 5.4813e-01, -2.5671e-01, -8.3601e+00],\n",
       "        [ 3.9612e-01, -1.6611e-01, -7.4256e+00],\n",
       "        [ 1.0893e-01,  1.6286e-01, -9.3442e+00],\n",
       "        [ 7.6036e-01, -4.6514e-01, -1.0472e+01],\n",
       "        [ 1.9329e-01,  3.4533e-03, -6.5389e+00],\n",
       "        [ 6.6558e-01, -3.8987e-01, -8.8520e+00],\n",
       "        [ 2.1970e-01, -1.9197e-03, -6.6199e+00],\n",
       "        [ 1.6921e+00, -5.9901e-01, -4.3492e+01],\n",
       "        [ 1.4489e+00, -7.2455e-01, -2.4093e+01],\n",
       "        [ 1.4234e-01,  2.8512e-01, -1.4927e+01],\n",
       "        [ 3.8332e-01, -1.5771e-01, -7.3385e+00],\n",
       "        [ 1.9191e+00, -1.0038e+00, -3.1869e+01],\n",
       "        [-7.2446e-01,  1.7029e+00, -3.7451e+01],\n",
       "        [ 5.0031e-01, -2.2225e-01, -8.0954e+00],\n",
       "        [ 6.1174e-01, -3.2348e-01, -8.5911e+00],\n",
       "        [ 1.9152e+00, -9.9996e-01, -3.0747e+01],\n",
       "        [ 6.8701e-01, -4.2694e-01, -8.3729e+00],\n",
       "        [ 4.3143e-01, -1.6852e-01, -7.8351e+00],\n",
       "        [ 6.0749e-01, -3.0195e-01, -8.7815e+00],\n",
       "        [ 7.0547e-01, -4.3855e-01, -8.4111e+00],\n",
       "        [ 2.9857e-02,  3.1112e-01, -1.2174e+01],\n",
       "        [ 5.4813e-01, -2.5671e-01, -8.3601e+00],\n",
       "        [ 1.1290e+00, -6.1624e-01, -1.7780e+01],\n",
       "        [ 9.2690e-01, -5.3242e-01, -1.4278e+01],\n",
       "        [ 8.1271e-01, -4.7571e-01, -1.1103e+01],\n",
       "        [ 6.9989e-01, -4.3310e-01, -8.6154e+00],\n",
       "        [ 5.8023e-01, -3.6029e-01, -7.1648e+00],\n",
       "        [ 3.5823e+00, -1.7582e+00, -5.9142e+01],\n",
       "        [ 9.6111e-01, -5.6344e-01, -1.4260e+01],\n",
       "        [-3.5259e-01,  1.0133e+00, -2.4306e+01],\n",
       "        [ 1.0750e+00, -6.1767e-01, -1.6482e+01]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPSTONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
